# ETL Project

# Purpose of Project:
This project purpose is to gather information on different TV shows on several cable netwroks and online streaming services like CBS, ABC, FOX, Netflix etc. 

# Project Summary: 
The project data are gatherred using different methods making API calls, web-scrapping the individual TV Show site and some open source flat files(.csv).
This data are cleaned, transformed and saved it into a Dataframe. These data are further combined with Show-info data and  existing API call data.
Few fields like cast, duration and  release year were missing. These fields were fetched from a flat file and were joined with the existing data. 

# Output Files:
There are three different output files:
1.  tvmaze_api_output.csv : This file has data extracted, cleaned and transformed from API calls .

2. tvmaze_api_webscrape_output.csv:  This output file has the web scrapped data combined witht the API call data. 

3. TVShows_final_output.csv: This file has the final output which is generated by joining the above outputs with the flat file data.

# Data Extraction Processes:

1. API Calls Process:
   The project starts with making call to TV Maze APIs. The data for all TV shows is extracted from the website. The data is for TV shows aired on different cable networks and online streaming platform from 1989-2014.
   Output file tvmaze_api_output.csv was generated in this process.

2. Web Scrapping Process: 
   In this process, individual TV show URL was called and scrapped using Beautiful Soup. Data like show summary, show images, ratings, created by, votes were fetched from each TV show web page. 
   This data was further cleaned and then merged with the APi process data and output tvmaze_api_webscrape_output.csv was generated.

3. Flat File Reading:
   In this process, a TV Shows.csv file was read locally( source: Kaggle) which had additional info about the TV shows like, cast, duration. This data was joined with the API and web scrapped data to generate the final output  which is saved in TVShows_final_output.csv

All the process followed the below cleaning and transformation steps before producing the final output.

# Cleaning  and Transformation of Data:

The initial data is collected  from a website by making APi calls. Data is returned in JSON format which is then converted and loaded to a Dataframe.

Below covers the DataFrame cleaning:

1. Cleaning up data: Data is to be cleaned to replace values. Few rows have data missing in parts, so replaced the empty data values to 'NA', removed  the unwanted characters in the dataframe .

2. Dropping the Duplicate Rows: Data is checked if there are any duplicates TV Shows. The TV show should be unique so it is indexed and any other duplicate TV show are dropped. 

3. Tidying/Transforming  Fields in the Data: Few columns had data in form of dictionary and dictionary of dictionaries. This data was further cleaned to seperate out the information 
   as individual fields  and store them as new columns. { E.g: New columns created by this process: Time, Day, Network, Country, Country Code, Timezone}

4. Dropping unwanted Columns: After the data was extracted in step 3, the columns with dictionaries were dropped to make a cleaner Dataframe. 

5. Rename Columns: The columns were renamed to appropriate column names.



# Loading the data:
 The data that was extracted, cleaned was further loaded in Mongo DB Database.

# Data  Limitation: 
Not all shows in TV Shows.csv  matched exactly to the tv shows from APi calls and web scrapping , such unmatched data was replaced by NA data.
  
